{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nbresult matplotlib==3.5.3 matplotlib-inline==0.1.6 numpy==1.23.4 pandas seaborn==0.11.2 scipy xgboost scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregue os imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display = 'diagram')\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.ensemble import AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression, VarianceThreshold, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜ Desafio em Lote do Kaggle\n",
    "============================\n",
    "\n",
    "**Bem-vindo √† sua primeira competi√ß√£o no Kaggle!**\n",
    "\n",
    "Seu objetivo √© **submeter uma resposta (online)** para a competi√ß√£o aberta [Pre√ßos de Casas - T√©cnicas Avan√ßadas de Regress√£o](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) üè†\n",
    "\n",
    "Voc√™ ser√° semi-orientado para um **modelo de base**, e somente ap√≥s criar esse modelo de base voc√™ estar√° livre para aprimor√°-lo e refin√°-lo. Abordaremos o problema usando **pipelines** (a melhor pr√°tica)!\n",
    "\n",
    "Algumas palavras sobre o Kaggle:\n",
    "\n",
    "* O Kaggle classificar√° sua submiss√£o entre todos os participantes!\n",
    "* Todos s√£o removidos do ranking p√∫blico ap√≥s 2 meses\n",
    "* Voc√™ pode fazer at√© 10 submiss√µes por dia\n",
    "\n",
    "üßπ Hoje √© o dia perfeito para praticar manter seu longo caderno **organizado** üßπ\n",
    "\n",
    "* Colapse todos os t√≠tulos a partir da paleta de comandos (`Cmd + Shift + P`)\n",
    "* Mantenha-se \"idempotente\" (`Restart & Run All` nunca deve falhar)\n",
    "* Nomeie e delete vari√°veis com cuidado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configura√ß√£o do Kaggle\n",
    "----------------------\n",
    "\n",
    "üëâ Crie uma conta no Kaggle se quiser participar da competi√ß√£o\n",
    "\n",
    "üëâ Junte-se ao [Desafio de Pre√ßos de Casas](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando Dados\n",
    "----------------\n",
    "\n",
    "Nas instru√ß√µes do desafio, voc√™ j√° deveria ter executado os passos para baixar tudo o que precisa do Kaggle para a pasta atual do seu notebook:\n",
    "\n",
    "* `train.csv` √© o seu conjunto de treinamento `(1460, 81)` contendo `X` e `y`\n",
    "* `test.csv` √© o seu conjunto de teste `(1459, 80)` sem o alvo associado `y` üòà\n",
    "* `sample_submission.csv` descreve o formato necess√°rio para submeter sua resposta\n",
    "\n",
    "Seu objetivo √© prever o `y_pred` que falta no seu conjunto de teste e submet√™-lo para descobrir seu `test_score` e classifica√ß√£o\n",
    "\n",
    "‚ùì Carregue o conjunto de dados de treinamento em um DataFrame chamado `data`, e crie seu `X` e `y`. Inspecione seus formatos.\n",
    "\n",
    "**Dica:** se voc√™ verificar o arquivo CSV, notar√° uma coluna chamada `Id`. Ao ler o arquivo CSV em um DF, certifique-se de definir `index_col=\"Id\"` para que voc√™ n√£o tenha duas colunas de ID üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Vamos fazer o load dos dados na vari√°vel 'data'\n",
    "data = pd.read_csv('data/houses_train_raw.csv', index_col='Id')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Vamos fazer o drop da coluna SalePrice\n",
    "\n",
    "X = data.drop(columns=['SalePrice'])\n",
    "y = data.SalePrice\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê£ 1. BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Vis√£o inicial das caracter√≠sticas\n",
    "-------------------------------------\n",
    "\n",
    "80 caracter√≠sticas s√£o demais para lidar individualmente para um primeiro pipeline de base! Vamos trat√°-las baseando-nos somente em seu `dtype`:\n",
    "\n",
    "‚ùì Quantas caracter√≠sticas num√©ricas versus caracter√≠sticas categ√≥ricas temos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Crie uma S√©rie chamada `feat_categorical_nunique` contendo o n√∫mero de **valores √∫nicos** para cada caracter√≠stica categ√≥rica no nosso conjunto de treinamento. Quantas categorias √∫nicas existem no total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "feat_categorical_nunique = X.select_dtypes(include='object').nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "feat_categorical_nunique.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Se f√¥ssemos para o `OneHotEncode`, em todas as caracter√≠sticas categ√≥ricas, nossa matriz de caracter√≠sticas `X_preproc` se tornaria bastante grande e esparsa, com quase 300 caracter√≠sticas (altamente correlacionadas) para apenas 1400 observa√ß√µes. Idealmente, dever√≠amos visar alimentar nosso modelo com um m√°ximo de ~50 caracter√≠sticas (üìö leia esta [regra pr√°tica](https://datascience.stackexchange.com/a/11480/98300))\n",
    "\n",
    "Conhecemos 2 principais estrat√©gias para reduzir o n√∫mero de caracter√≠sticas categ√≥ricas ap√≥s o pr√©-processamento:\n",
    "\n",
    "1. **[Remover](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)** caracter√≠sticas que trazem pouca explica√ß√£o para nosso modelo; isso pode exigir an√°lise estat√≠stica da import√¢ncia das caracter√≠sticas\n",
    "2. **[Codificar ordinalmente](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** (em vez de usar one-hot encode) caracter√≠sticas categ√≥ricas em inteiros; isso, no entanto, cria uma no√ß√£o de \"ordem\" (1 > 2 > 3 > ...) que pode ser prejudicial se n√£o for manuseada corretamente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plote o histograma do n√∫mero de valores √∫nicos por caracter√≠stica categ√≥rica. Voc√™ v√™ alguns quick wins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "feat_categorical_nunique.hist();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Como ponto de partida, que tal simplesmente remover todas as caracter√≠sticas que t√™m 7 valores √∫nicos ou mais e aplicar a codifica√ß√£o one-hot no restante? Vamos manter a codifica√ß√£o ordinal e a sele√ß√£o de caracter√≠sticas estat√≠sticas para a pr√≥xima itera√ß√£o do nosso pipeline.\n",
    "\n",
    "‚ùì Armazene os nomes das caracter√≠sticas a serem codificadas em one-hot em uma lista chamada feat_categorical_small abaixo. Quantas caracter√≠sticas ser√£o codificadas em one-hot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# categorical features para one-hot-encode\n",
    "feat_categorical_small = list(feat_categorical_nunique[feat_categorical_nunique < 7].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Quantidade de catgorias\n",
    "len(feat_categorical_small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Teste o c√≥digo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'features_overview',\n",
    "    n=len(feat_categorical_small)\n",
    ")\n",
    "\n",
    "result.write()\n",
    "result.check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Baseline Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Pr√©-processamento\n",
    "\n",
    "‚ùì Vamos codificar a linha de base do pr√©-processamento conforme descrito abaixo. Salve-a como `preproc_baseline`.\n",
    "\n",
    "Para caracter√≠sticas categ√≥ricas:\n",
    "\n",
    "* Impute simples com os valores mais frequentes\n",
    "* Codifica√ß√£o One-Hot para caracter√≠sticas que t√™m menos de 7 valores √∫nicos inicialmente\n",
    "* Remova todas as outras caracter√≠sticas\n",
    "\n",
    "Quanto √†s caracter√≠sticas num√©ricas:\n",
    "\n",
    "* Impute simples com estrat√©gia `m√©dia`\n",
    "* Escala Min-Max\n",
    "\n",
    "<details> <summary>‚ÑπÔ∏è Clique aqui para uma dica profissional</summary>\n",
    "\n",
    "Se estiver confiante, voc√™ pode tentar a sintaxe mais curta do Sklearn, como `make_pipeline` ou `make_column_transformer`, em vez da sintaxe mais longa de `Pipeline` ou `ColumnTransformer`; tamb√©m √© √∫til se voc√™ quiser evitar dar nomes manualmente a cada etapa.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "preproc_numerical_baseline = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    MinMaxScaler()\n",
    ")\n",
    "\n",
    "preproc_categorical_baseline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "preproc_baseline = make_column_transformer(\n",
    "    (preproc_numerical_baseline, make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
    "    (preproc_categorical_baseline, feat_categorical_small),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preproc_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Observe a **forma** do seu DataFrame ap√≥s o pr√©-processamento e salve-a como `shape_preproc_baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "shape_preproc_baseline = preproc_baseline.fit_transform(X).shape\n",
    "shape_preproc_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'preproc_baseline',\n",
    "    shape=shape_preproc_baseline\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Add Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Adicione um modelo simples de √Årvore de Decis√£o ao seu preproc_baseline e armazene-o na vari√°vel pipe_baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_baseline = make_pipeline(preproc_baseline, DecisionTreeRegressor())\n",
    "pipe_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Cross-Valida√ß√£o\n",
    "\n",
    "‚ùì Leia as [regras de avalia√ß√£o do concurso Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation). Qual m√©trica de desempenho voc√™ precisa? Ela est√° prontamente dispon√≠vel no Sklearn?\n",
    "\n",
    "Infelizmente, n√£o est√°! Precisaremos criar nosso objeto personalizado `sklearn.metrics.scorer` para passar para qualquer valida√ß√£o cruzada ou pesquisa em grade. O processo √© descrito abaixo:\n",
    "\n",
    "1. Crie um scorer chamado `rmsle` usando [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) que pode ser passado como valor para o `kwarg` `scoring` da seguinte maneira:\n",
    "    \n",
    "    ```python\n",
    "    cross_val_score(pipe_baseline, X, y, cv=5, scoring=rmsle)\n",
    "    ```\n",
    "    \n",
    "2. Crie sua contraparte negativa, `rmsle_neg`, que √© melhor quando _maximizada_; isso ser√° √∫til mais tarde, j√° que o `GridSearchCV` sempre tenta _maximizar_ uma pontua√ß√£o üòâ\n",
    "    \n",
    "    ```python\n",
    "    GridSearchCV(pipe_baseline, param_grid=..., cv=5, scoring=rmsle_neg)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSLE formula\n",
    "\n",
    "$$\\text{RMSLE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# OPTION 1: recode it all manually\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    t = np.array(y_true)\n",
    "    p = np.array(y_pred)\n",
    "\n",
    "    log_error = np.log(1+t) - np.log(1+p)\n",
    "\n",
    "    return ((log_error**2).mean())**0.5\n",
    "\n",
    "# This is our metric to minimize\n",
    "rmsle = make_scorer(root_mean_squared_log_error)\n",
    "\n",
    "# This is our score to maximize\n",
    "rmsle_neg = make_scorer(lambda y_true, y_pred: -1 * root_mean_squared_log_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# OPTION 2 - re-use Sklearn's \"mean_squared_log_error\"\n",
    "\n",
    "# This is our metric to minimize\n",
    "rmsle = make_scorer(lambda y_true, y_pred: mean_squared_log_error(y_true, y_pred)**0.5)\n",
    "\n",
    "# This is our score to maximize\n",
    "rmsle_neg = make_scorer(lambda y_true, y_pred: -1 * mean_squared_log_error(y_true, y_pred)**0.5)\n",
    "\n",
    "# Equivalent formulation\n",
    "rmsle_neg = make_scorer(\n",
    "    lambda y_true, y_pred: mean_squared_log_error(y_true, y_pred)**0.5,\n",
    "    greater_is_better=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Realize a valida√ß√£o cruzada de 5 folds no seu pipe_baseline usando essa m√©trica para obter uma primeira vis√£o do desempenho b√°sico.\n",
    "\n",
    "Armazene a pontua√ß√£o m√©dia como score_baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "score_baseline = cross_val_score(pipe_baseline, X, y, cv=5, scoring=rmsle).mean()\n",
    "score_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Predict Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Fa√ßa previs√µes (y_pred_baseline) no conjunto de dados Kaggle test.csv que voc√™ armazenou na pasta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"data/houses_test_raw.csv\")\n",
    "X_test_ids = X_test['Id'] # Keep ids\n",
    "X_test = X_test.drop(columns=['Id'])\n",
    "\n",
    "# Predict y_pred_baseline\n",
    "pipe_baseline.fit(X,y)\n",
    "y_pred_baseline = pipe_baseline.predict(X_test)\n",
    "y_pred_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Por fim, armazene o seu arquivo CSV pronto para envio como submission_baseline.csv na pasta data. Leia cuidadosamente e compreenda o formato necess√°rio do Kaggle e teste abaixo (voc√™ n√£o precisa enviar esta linha de base para o Kaggle por enquanto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "results = pd.concat([X_test_ids, pd.Series(y_pred_baseline, name=\"SalePrice\")], axis=1)\n",
    "results.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Exporte os resultados\n",
    "results.to_csv(\"data/submission_baseline.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "tmp = pd.read_csv(\"data/submission_baseline.csv\")\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'submission_baseline',\n",
    "    score_baseline = score_baseline,\n",
    "    submission_shape = tmp.shape,\n",
    "    submission_columns = list(tmp.columns),\n",
    "    submission_dtypes = str(list(tmp.dtypes)),\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèãÔ∏è‚Äç‚ôÄÔ∏è 2. ITERA√á√ïES\n",
    "===================\n",
    "\n",
    "üéâ üéâ Parab√©ns por ter completamente constru√≠do um modelo b√°sico! Agora, voc√™ ver√° o qu√£o mais f√°cil √© iterar e melhorar o desempenho üöÄ\n",
    "\n",
    "Agora, seu objetivo √© melhorar suas previs√µes e envi√°-las para o Kaggle **pelo menos 30 minutos antes do Resumo ‚è≥**\n",
    "\n",
    "Temos algumas sugest√µes de melhorias abaixo: **escolha suas batalhas** e melhore **incrementalmente** seu pipeline conforme achar adequado!\n",
    "\n",
    "**Estimadores**\n",
    "\n",
    "* Conjuntos baseados em √°rvores (um must-try hoje); provavelmente os mais adequados para problemas com muitas caracter√≠sticas categ√≥ricas\n",
    "* Stacking!\n",
    "* XGBoost!\n",
    "\n",
    "**Pr√©-processamento** (quando seu primeiro modelo de conjunto estiver funcionando)\n",
    "\n",
    "* **Codifica√ß√£o Ordinal** de caracter√≠sticas categ√≥ricas com uma no√ß√£o oculta de ordem em seus valores (por exemplo, \"ruim\", \"m√©dio\", \"bom\")\n",
    "* **Sele√ß√£o Estat√≠stica de Caracter√≠sticas** para remover caracter√≠sticas in√∫teis (evita overfitting e reduz o tempo de treinamento)\n",
    "* Prever `log(SalePrice)` em vez disso?\n",
    "* ü§∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Itera√ß√£o de Pr√©-processamento ‚ô≤\n",
    "-----------------------------------\n",
    "\n",
    "**‚ö†Ô∏è Volte aqui apenas depois de ter iterado nos seus estimadores na se√ß√£o 2.2 ‚ö†Ô∏è**\n",
    "\n",
    "‚è© Me colapse se eu n√£o estiver sendo usado!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Ordinal Encoding (~1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Olhe para a seguinte caracter√≠stica. N√£o poderia ser codificada de maneira inteligente numericamente?\n",
    "\n",
    "```perl\n",
    "ExterQual: Avalia a qualidade do material no exterior\n",
    "\t\t\n",
    "       Ex\tExcelente\n",
    "       Gd\tBom\n",
    "       TA\tM√©dia/T√≠pica\n",
    "       Fa\tRuim\n",
    "       Po\tP√©ssimo\n",
    "```\n",
    "\n",
    "üí° Felizmente, o `OrdinalEncoder` e seu argumento `categories` nos permite fazer exatamente isso! Confira abaixo e certifique-se de entender como isso funciona üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uma ordem espec√≠fica para as caracter√≠sticas\n",
    "# Observa√ß√£o: se voc√™ alterar esta ordem, ela mudar√° a sa√≠da para .transform()\n",
    "feature_A_sorted_values = ['bad', 'average', 'good']\n",
    "feature_B_sorted_values = ['dirty', 'clean', 'new']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        feature_A_sorted_values,\n",
    "        feature_B_sorted_values\n",
    "    ],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Just some random training data\n",
    "XX = [\n",
    "    ['good', 'dirty'],\n",
    "    ['bad', 'new'],\n",
    "    ['average', 'clean'],\n",
    "]\n",
    "\n",
    "encoder.fit(XX)\n",
    "\n",
    "encoder.transform([\n",
    "        ['bad', \"dirty\"],\n",
    "        [\"average\", \"clean\"],\n",
    "        ['good', 'new'],\n",
    "        ['bad', 'oops never seen this label before']\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Sua vez**: divida seu pr√©-processador categ√≥rico em\n",
    "\n",
    "* `preproc_ordinal` para codificar de forma ordinal **algumas caracter√≠sticas** (de sua escolha)\n",
    "* `preproc_nominal` para codificar one-hot as outras\n",
    "\n",
    "<details> <summary>Dicas</summary>\n",
    "\n",
    "* Voc√™ n√£o conseguir√° evitar a codifica√ß√£o direta dos nomes e valores ordenados das caracter√≠sticas! Seja organizado!\n",
    "* √â uma boa pr√°tica ordenar suas caracter√≠sticas em ordem alfab√©tica para evitar surpresas desagrad√°veis.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "feat_ordinal_dict = {\n",
    "    # Considers \"missing\" as \"neutral\"\n",
    "    \"BsmtCond\": ['missing', 'Po', 'Fa', 'TA', 'Gd'],\n",
    "    \"BsmtExposure\": ['missing', 'No', 'Mn', 'Av', 'Gd'],\n",
    "    \"BsmtFinType1\": ['missing', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    \"BsmtFinType2\": ['missing', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    \"BsmtQual\": ['missing', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"Electrical\": ['missing', 'Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'],\n",
    "    \"ExterCond\": ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"ExterQual\": ['missing', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"Fence\": ['missing', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n",
    "    \"FireplaceQu\": ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"Functional\": ['missing', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "    \"GarageCond\": ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"GarageFinish\": ['missing', 'Unf', 'RFn', 'Fin'],\n",
    "    \"GarageQual\": ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"HeatingQC\": ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"KitchenQual\": ['missing', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    \"LandContour\": ['missing', 'Low', 'Bnk', 'HLS', 'Lvl'],\n",
    "    \"LandSlope\": ['missing', 'Sev', 'Mod', 'Gtl'],\n",
    "    \"LotShape\": ['missing', 'IR3', 'IR2', 'IR1', 'Reg'],\n",
    "    \"PavedDrive\": ['missing', 'N', 'P', 'Y'],\n",
    "    \"PoolQC\": ['missing', 'Fa', 'Gd', 'Ex']\n",
    "}\n",
    "\n",
    "feat_ordinal = sorted(feat_ordinal_dict.keys()) # sort alphabetically\n",
    "feat_ordinal_values_sorted = [feat_ordinal_dict[i] for i in feat_ordinal]\n",
    "\n",
    "encoder_ordinal = OrdinalEncoder(\n",
    "    categories=feat_ordinal_values_sorted,\n",
    "    dtype= np.int64,\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1 # Considers unknown values as worse than \"missing\"\n",
    ")\n",
    "\n",
    "preproc_ordinal = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    encoder_ordinal,\n",
    "    MinMaxScaler()\n",
    ")\n",
    "\n",
    "preproc_ordinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete_begin"
    ]
   },
   "outputs": [],
   "source": [
    "# Define caracter√≠sticas num√©ricas de uma vez por todas.\n",
    "\n",
    "feat_numerical = sorted(X.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n",
    "\n",
    "preproc_numerical = make_pipeline(\n",
    "    KNNImputer(),\n",
    "    MinMaxScaler()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define caracter√≠sticas nominais para codificar one-hot como as restantes (n√£o num√©ricas, n√£o ordinais)\n",
    "feat_nominal = sorted(list(set(X.columns) - set(feat_numerical) - set(feat_ordinal)))\n",
    "\n",
    "preproc_nominal = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = make_column_transformer(\n",
    "    (preproc_numerical, feat_numerical),\n",
    "    (preproc_ordinal, feat_ordinal),\n",
    "    (preproc_nominal, feat_nominal),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete_end"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preproc.fit_transform(X,y)).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Sele√ß√£o Estat√≠stica de Caracter√≠sticas (~30min)\n",
    "\n",
    "Nosso objetivo √© remover as caracter√≠sticas menos interessantes para limitar o overfitting e reduzir o tempo de treinamento.\n",
    "\n",
    "üî• Vamos fazer uso dos transformadores de [sele√ß√£o de caracter√≠sticas](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) do Sklearn diretamente no seu pipeline!\n",
    "\n",
    "‚ùóÔ∏è Recomendamos que voc√™ tente **apenas a Op√ß√£o 1 hoje**, para come√ßar. As Op√ß√µes 2 e 3 ser√£o corrigidas no Resumo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Op√ß√£o 1 (Recomendada) - <font color=green>Sele√ß√£o de Caracter√≠sticas Univariada</font>\n",
    "\n",
    "_com base na informa√ß√£o m√∫tua com o alvo `y`_\n",
    "\n",
    "* Sinta-se √† vontade para adicionar um filtro `SelectPercentile` ao final do seu pipeline `preproc`.\n",
    "* Isso filtrar√° caracter√≠sticas que, individualmente, explicam menos o seu alvo!\n",
    "* O teste estat√≠stico que recomendamos passar para o `SelectPercentile` √© o `mutual_info_regression`\n",
    "\n",
    "<details> <summary markdown='span'>ü§î O que √© informa√ß√£o m√∫tua? Clique aqui!</summary>\n",
    "\n",
    "* [Informa√ß√£o M√∫tua](https://en.wikipedia.org/wiki/Mutual_information) √© uma dist√¢ncia **estat√≠stica** entre duas distribui√ß√µes de probabilidade.\n",
    "* A correla√ß√£o √© uma dist√¢ncia **linear** entre duas vari√°veis aleat√≥rias.\n",
    "* A Informa√ß√£o M√∫tua √© mais geral e mede a redu√ß√£o de incerteza em Y ap√≥s observar X.\n",
    "* Por outro lado, se voc√™ j√° sabe que est√° lidando com vari√°veis suaves (como vari√°veis num√©ricas cont√≠nuas), √†s vezes a correla√ß√£o pode fornecer mais informa√ß√µes sobre elas, por exemplo, se a rela√ß√£o entre elas for mon√≥tona.\n",
    "\n",
    "Veja [esta anima√ß√£o](https://twitter.com/ari_seff/status/1409296508634152964)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "preproc_transformer = make_column_transformer(\n",
    "    (preproc_numerical, make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
    "    (preproc_ordinal, feat_ordinal),\n",
    "    (preproc_nominal, feat_nominal),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preproc_selector = SelectPercentile(\n",
    "    mutual_info_regression,\n",
    "    percentile=25, # keep only 25% of all features\n",
    ")\n",
    "\n",
    "preproc = make_pipeline(\n",
    "    preproc_transformer,\n",
    "    preproc_selector\n",
    ")\n",
    "\n",
    "preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "preproc.fit_transform(X, y).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Op√ß√£o 2 - <font color=green>Sele√ß√£o de Caracter√≠sticas Multivariada</font>\n",
    "\n",
    "_com base em sua rela√ß√£o combinada com o alvo `y`_\n",
    "\n",
    "ü§î Queremos remover caracter√≠sticas que n√£o ajudam a prever nosso alvo mesmo quando combinadas com todas as outras.\n",
    "\n",
    "1Ô∏è‚É£ Para fazer isso, lembre-se de que podemos usar a m√©trica [`permutation_importance`](https://scikit-learn.org/stable/modules/permutation_importance.html) em combina√ß√£o com um estimador! Ele treina um pipeline por caracter√≠stica para estimar qual caracter√≠stica faz nosso escore de desempenho _diminuir_ mais quando ela √© embaralhada aleatoriamente. Essas seriam nossas caracter√≠sticas mais importantes, que n√£o queremos remover.\n",
    "\n",
    "A melhor parte √© que o `scikit-learn` permite que voc√™ integre essa metodologia diretamente no seu pipeline `preproc` gra√ßas ao transformador [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html); isso remover√° recursivamente as caracter√≠sticas menos importantes de acordo com o `cross_val_score`.\n",
    "\n",
    "Quando voc√™ tem muitas caracter√≠sticas, no entanto, esse processo pode levar muito tempo para treinar.\n",
    "\n",
    "2Ô∏è‚É£ Alternativamente, uma maneira mais r√°pida seria fazer uso de modelos que j√° produzem alguma medida de `feature_importance` ao serem ajustados. Por exemplo, √°rvores com `feature_importance_` baseado em Gini, ou regress√µes Lasso com `coef_` L1. O `scikit-learn` j√° possui o transformador [`SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) para fazer exatamente isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "preproc_transformer_multi = make_column_transformer(\n",
    "    (preproc_numerical, make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
    "    (preproc_ordinal, feat_ordinal),\n",
    "    (preproc_nominal, feat_nominal),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preproc_selector_multi = SelectFromModel(\n",
    "    RandomForestRegressor(),\n",
    "    threshold = \"median\", # drop all multivariate features lower than the median correlation\n",
    ")\n",
    "\n",
    "preproc_multi = make_pipeline(\n",
    "    preproc_transformer_multi,\n",
    "    preproc_selector_multi\n",
    ")\n",
    "\n",
    "preproc_multi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Op√ß√£o 3 - Sele√ß√£o <font color=green>N√£o Supervisionada</font>?\n",
    "\n",
    "_filtre baseado apenas nas propriedades de `X`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Uma vit√≥ria r√°pida √© remover caracter√≠sticas com a menor vari√¢ncia. Pense sobre isso: uma caracter√≠stica que s√≥ tem um valor √© in√∫til (e tem uma vari√¢ncia de 0).\n",
    "\n",
    "Sinta-se √† vontade para adicionar um [`VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) ao final do seu pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def number_feature_remaining(cutoff=0):\n",
    "    preproc_transformer = make_column_transformer(\n",
    "        (preproc_numerical, feat_numerical),\n",
    "        (preproc_ordinal, feat_ordinal),\n",
    "        (preproc_nominal, feat_nominal),\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    preproc_selector = VarianceThreshold(cutoff)\n",
    "\n",
    "    preproc = make_pipeline(\n",
    "        preproc_transformer,\n",
    "        preproc_selector\n",
    "    )\n",
    "\n",
    "    return preproc.fit_transform(X).shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "cutoff_values = np.arange(0, 0.2, 0.01)\n",
    "\n",
    "plt.plot(cutoff_values, [number_feature_remaining(t) for t in cutoff_values], marker='x')\n",
    "\n",
    "plt.xlabel(\"chosen feature variance cutoff values\")\n",
    "plt.title(\"Number of Feature Remaining\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Poder√≠amos decidir colocar um limite de 0.025 nas caracter√≠sticas categ√≥ricas para reduzir o n√∫mero delas pela metade ou mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Al√©m disso, podemos verificar a correla√ß√£o entre nossas **caracter√≠sticas num√©ricas** apenas\n",
    "\n",
    "* Use a [correla√ß√£o de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) combinada com um mapa de calor para verificar visualmente se alguma caracter√≠stica **num√©rica** se correlaciona quase que inteiramente com outras\n",
    "* Use o `VIF` de `statsmodels` para verificar quais caracter√≠sticas t√™m a maior multicolinearidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "corr_num = X[feat_numerical].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_num, cmap='coolwarm',vmin=-1, vmax=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Verifique quais colunas remover com base na alta correla√ß√£o\n",
    "num_corr_threshold = 0.95\n",
    "\n",
    "corr_num = X[feat_numerical].corr()\n",
    "corr_num_upper_triangle = corr_num.where(np.triu(np.ones(corr_num.shape), k=1).astype(np.bool)).abs()\n",
    "\n",
    "num_col_to_drop = [column for column in corr_num_upper_triangle.columns if any(corr_num_upper_triangle[column] > num_corr_threshold)]\n",
    "num_col_to_drop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Para **caracter√≠sticas ordinais**, podemos usar a [correla√ß√£o de postos de Spearman](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) para verificar se algumas caracter√≠sticas **codificadas ordinalmente** s√£o quase inteiramente \"ordenadas\" de maneira semelhante √†s outras. Sinta-se √† vontade para plotar um mapa de calor novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_ordinally_encoded = pd.DataFrame(preproc_ordinal.fit_transform(X[feat_ordinal]))\n",
    "\n",
    "sns.heatmap(X_ordinally_encoded.corr(method='spearman'), cmap='coolwarm', vmin=-1, vmax=1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Agora, sinta-se √† vontade para criar um \"filtro\" em seu pipeline que remove qualquer caracter√≠stica al√©m de um determinado limite de correla√ß√£o (Spearman + Pearson); voc√™ precisar√° de uma classe de transformador personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "class CustomFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_corr_threshold=0.95):\n",
    "        self.num_corr_threshold = num_corr_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feat_numerical = sorted(X.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n",
    "\n",
    "        corr_num = X[feat_numerical].corr()\n",
    "        upper = corr_num.where(np.triu(np.ones(corr_num.shape), k=1).astype(np.bool)).abs()\n",
    "\n",
    "        self.num_col_to_drop = [column for column in upper.columns if any(upper[column] > self.num_corr_threshold)]\n",
    "        self.num_col = X[feat_numerical].columns\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = pd.DataFrame(X, columns=self.num_col)\n",
    "\n",
    "        return df.drop(columns=self.num_col_to_drop)\n",
    "\n",
    "# Test it here\n",
    "CustomFeatureSelector(num_corr_threshold=0.2).fit_transform(X[feat_numerical]).head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "source": [
    "### ü•∑ Solu√ß√£o Apenas: Outras Transforma√ß√µes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Tratar Caracter√≠sticas C√≠clicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Temos algumas caracter√≠sticas relacionadas ao tempo, por que n√£o transform√°-las em caracter√≠sticas c√≠clicas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Tratar Caracter√≠sticas C√≠clicas\n",
    "months_in_a_year = 12\n",
    "\n",
    "X['sin_MoSold'] = np.sin(2 * np.pi * (X.MoSold - 1) / months_in_a_year)\n",
    "X['cos_MoSold'] = np.cos(2 * np.pi * (X.MoSold - 1) / months_in_a_year)\n",
    "\n",
    "X.drop(columns=['MoSold'], inplace=True)\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Engenharia do Alvo (~15min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Nos foi pedido para minimizar o RMS**L**E. Que tal transformarmos nosso alvo para prever diretamente seu `log`?\n",
    "\n",
    "* Confira o histograma do alvo `y`\n",
    "* Vari√°veis normalmente distribu√≠das devem ser mais f√°ceis de prever com modelos lineares ou param√©tricos\n",
    "* Crie `y_log` e suas novas m√©tricas de desempenho\n",
    "* N√£o se esque√ßa de tomar a exponencial de suas previs√µes no final!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "y_log = np.log(y)\n",
    "\n",
    "plt.figure(figsize=(17, 5))\n",
    "\n",
    "# Subplot para o histograma original\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Histograma de SalePrice')\n",
    "plt.xlabel('SalePrice')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "\n",
    "# Subplot para o histograma dos dados transformados\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_log, bins=30, color='green', edgecolor='black')\n",
    "plt.title('Histograma do log(SalePrice)')\n",
    "plt.xlabel('log(SalePrice)')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Crie seu novo marcador para minimizar\n",
    "rmse = make_scorer(lambda y_true, y_pred: mean_squared_error(y_true, y_pred)**0.5)\n",
    "\n",
    "# Crie seu novo artilheiro para maximizar\n",
    "rmse_neg = make_scorer(lambda y_true, y_pred: -1 * mean_squared_error(y_true, y_pred)**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Iteration ‚ôª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Vers√£o Final do Pipeline de Pr√©-processamento\n",
    "\n",
    "‚ùì Aconselhamos que voc√™ comece com uma defini√ß√£o nova abaixo para que voc√™ possa atualiz√°-la rapidamente conforme necess√°rio e, em seguida, experimentar muitos tipos de modelos para encontrar o melhor poss√≠vel (voc√™ pode tentar GridSearch ou ir modelo por modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "encoder_ordinal = OrdinalEncoder(\n",
    "    categories=feat_ordinal_values_sorted,\n",
    "    dtype= np.int64,\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1 # Considers unknown values as worse than \"missing\"\n",
    ")\n",
    "\n",
    "preproc_ordinal = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    encoder_ordinal,\n",
    "    MinMaxScaler()\n",
    ")\n",
    "\n",
    "preproc_numerical = make_pipeline(\n",
    "    KNNImputer(),\n",
    "    MinMaxScaler()\n",
    ")\n",
    "\n",
    "preproc_transformer = make_column_transformer(\n",
    "    (preproc_numerical, make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
    "    (preproc_ordinal, feat_ordinal),\n",
    "    (preproc_nominal, feat_nominal),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preproc_selector = SelectPercentile(\n",
    "    mutual_info_regression,\n",
    "    percentile=50, # keep only xx% of all features )\n",
    ")\n",
    "preproc = make_pipeline(\n",
    "    preproc_transformer,\n",
    "    preproc_selector\n",
    ")\n",
    "\n",
    "preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Check shape\n",
    "preproc_fitted = preproc.fit(X,y)\n",
    "preproc_fitted_log = preproc.fit(X,y_log)\n",
    "\n",
    "preproc_fitted_log.transform(X).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUCAO\n",
    "allow_grid_searching = True # Use True para ativar o GridSearch nas c√©lulas do notebook abaixo\n",
    "\n",
    "# Armazenar em cache a etapa de pr√©-processamento do pipeline\n",
    "cachedir = mkdtemp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "delete_begin"
    ]
   },
   "source": [
    "#### b) Modelos Lineares (Lasso, Ridge, ElasticNet, SGDRegressor, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge com normal target\n",
    "model = Ridge()\n",
    "\n",
    "pipe_ridge = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "cross_val_score(pipe_ridge, X, y, cv=5, scoring=rmsle).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge com log-target (much better)\n",
    "model = Ridge()\n",
    "\n",
    "pipe_ridge = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "cross_val_score(pipe_ridge, X, y_log, cv=5, scoring=rmse).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch the Ridge regularization\n",
    "if allow_grid_searching:\n",
    "    param_grid =  {'ridge__alpha': np.linspace(0.5, 2, num=20)}\n",
    "\n",
    "    search_ridge = GridSearchCV(\n",
    "        pipe_ridge,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring=rmse_neg\n",
    "    )\n",
    "\n",
    "    search_ridge.fit(X, y_log);\n",
    "\n",
    "    print('\\n----------------------------------------\\n')\n",
    "    print(f'Best params üëâ {search_ridge.best_params_}')\n",
    "    print(f'Best score üëâ {search_ridge.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor()\n",
    "\n",
    "pipe_knn = make_pipeline(preproc, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipe_knn, X, y_log, cv=5, scoring=rmse)\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch the KNN\n",
    "if allow_grid_searching:\n",
    "    param_grid =  {'kneighborsregressor__n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30]}\n",
    "\n",
    "    search_knn = GridSearchCV(\n",
    "        pipe_knn,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring=rmse_neg\n",
    "    )\n",
    "\n",
    "    search_knn.fit(X, y_log);\n",
    "\n",
    "    print('\\n----------------------------------------\\n')\n",
    "    print(f'Best params üëâ {search_knn.best_params_}')\n",
    "    print(f'Best score üëâ {search_knn.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='linear')\n",
    "\n",
    "pipe_svm = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "cross_val_score(pipe_svm, X, y_log, cv=5, scoring=rmse).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='rbf', C = 10)\n",
    "\n",
    "pipe_svm_rbf = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "cross_val_score(pipe_svm_rbf, X, y_log, cv=5, scoring=rmse).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "if allow_grid_searching:\n",
    "    param_grid =  {\n",
    "        'svr__C': [0.5, 0.7, 1, 2, 5, 10],\n",
    "        'svr__epsilon': [0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "        #'svr__coef0': [0.0, 0.1, 0.5,1],\n",
    "    }\n",
    "\n",
    "    search_svm_rbf = GridSearchCV(\n",
    "        pipe_svm_rbf,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring=rmse_neg\n",
    "    )\n",
    "\n",
    "    search_svm_rbf.fit(X, y_log);\n",
    "\n",
    "    svm_rbf_best = search_svm_rbf.best_estimator_\n",
    "\n",
    "    print('\\n----------------------------------------\\n')\n",
    "    print(f'Best params üëâ {search_svm_rbf.best_params_}')\n",
    "    print(f'Best score üëâ {search_svm_rbf.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) √Årvores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=50, min_samples_leaf=20)\n",
    "\n",
    "pipe = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "score = cross_val_score(pipe, X, y_log, cv=5, scoring=rmse)\n",
    "\n",
    "print(score.std())\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Floresta Aleat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=50,min_samples_leaf=20)\n",
    "\n",
    "pipe = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "score = cross_val_score(pipe, X, y_log, cv=5, scoring=rmse)\n",
    "\n",
    "print(score.std())\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g) Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None))\n",
    "\n",
    "pipe = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "score = cross_val_score(pipe, X, y_log, cv=5, scoring=rmse)\n",
    "\n",
    "print(score.std())\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=100, verbose=0)\n",
    "\n",
    "pipe_gb = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "score = cross_val_score(pipe_gb, X, y_log, cv=5, scoring=rmse)\n",
    "\n",
    "print(score.std())\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allow_grid_searching:\n",
    "     grid = {\n",
    "          'gradientboostingregressor__n_estimators': stats.randint(50,300),\n",
    "          # 'gradientboostingregressor__learning_rate': stats.uniform(0.05, 0.3),\n",
    "          # 'gradientboostingregressor__loss': ['lad', 'huber', 'quantile'],\n",
    "          # 'gradientboostingregressor__max_depth': stats.randint(3, 5),\n",
    "          # 'gradientboostingregressor__min_samples_split': stats.randint(2, 10),\n",
    "          # 'gradientboostingregressor__subsample': [0.95, 1], # 1 default\n",
    "          'gradientboostingregressor__max_features': stats.randint(0.9, len(X.columns)) # default None, i.e = n_features\n",
    "     }\n",
    "\n",
    "     search_gb = RandomizedSearchCV(pipe_gb, grid, scoring=rmse_neg, n_iter=8, cv=5, n_jobs=1, verbose=2)\n",
    "\n",
    "     # Fit data to GridSearch\n",
    "     search_gb.fit(X, y_log);\n",
    "\n",
    "     print('\\n----------------------------------------\\n')\n",
    "     print(f'Best params üëâ {search_gb.best_params_}')\n",
    "     print(f'Best score üëâ {search_gb.best_score_}')\n",
    "\n",
    "     # Plot results of GridSearch\n",
    "     df_cv_results_ = pd.DataFrame(search_gb.cv_results_)\n",
    "\n",
    "     sns.scatterplot(x=\"param_gradientboostingregressor__n_estimators\", y='mean_test_score', data=df_cv_results_)\n",
    "     sns.scatterplot(x=\"param_gradientboostingregressor__max_features\", y='mean_test_score', data=df_cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h) Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingRegressor(n_estimators=100)\n",
    "ridge = Ridge()\n",
    "svm = SVR(C=1, epsilon=0.05)\n",
    "adaboost = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None))\n",
    "\n",
    "\n",
    "model = VotingRegressor(\n",
    "    estimators = [(\"gboost\", gboost), (\"adaboost\", adaboost), (\"ridge\", ridge), (\"svm_rbf\", svm)],\n",
    "    weights = [1, 1, 1, 1], # to equally weight the models\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe_ensemble = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "score = cross_val_score(pipe_ensemble, X, y_log, cv=5, scoring=rmse, n_jobs=-1)\n",
    "\n",
    "print(score.std())\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingRegressor(n_estimators=100)\n",
    "ridge = Ridge()\n",
    "svm = SVR(C=1, epsilon=0.05)\n",
    "adaboost = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None))\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=[(\"gboost\", gboost), (\"adaboost\", adaboost), (\"ridge\", ridge), (\"svm_rbf\", svm)],\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe_stacking = make_pipeline(preproc, model, memory=cachedir)\n",
    "\n",
    "score = cross_val_score(pipe_stacking, X, y_log, cv=5, scoring=rmse, n_jobs=-1)\n",
    "\n",
    "print(score.std())\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um teste de avalia√ß√£o apenas para fins de parada antecipada (XGBOOST e Deep Learning)\n",
    "X_train, X_eval, y_train_log, y_eval_log = train_test_split(X, y_log, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar modelo\n",
    "model_xgb = XGBRegressor(max_depth=10, n_estimators=300, learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op√ß√£o 1: Integrar XGB ao pipeline do Sklearn\n",
    "# Permite GridSearchCV seus melhores hiperpar√¢metros\n",
    "pipe_xgb = make_pipeline(preproc, model_xgb)\n",
    "\n",
    "cross_val_score(pipe_xgb, X, y_log, cv=5, scoring=rmse, n_jobs=-1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete_end"
    ]
   },
   "outputs": [],
   "source": [
    "# Op√ß√£o 2: Use a biblioteca XGBoost para ajust√°-la\n",
    "# Permite que voc√™ use um crit√©rio `early_stopping` com uma fenda Train/Val\n",
    "X_train_preproc = preproc.fit_transform(X_train, y_train_log)\n",
    "X_eval_preproc = preproc.transform(X_eval)\n",
    "\n",
    "model_xgb.fit(\n",
    "    X_train_preproc,\n",
    "    y_train_log,\n",
    "    verbose=False,\n",
    "    eval_set=[(X_train_preproc, y_train_log), (X_eval_preproc, y_eval_log)],\n",
    "    eval_metric=[\"rmse\"],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Retrieve performance metrics\n",
    "results = model_xgb.evals_result()\n",
    "epochs = len(results['validation_0'][\"rmse\"])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# Plot RMSLE loss\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'], label='Val')\n",
    "ax.legend(); plt.ylabel('RMSE (of log)'); plt.title('XGBoost Log Loss')\n",
    "\n",
    "print(\"Best Validation Score\", min(results['validation_1']['rmse']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÖ APRESENTA√á√ÉO FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descubra sua pontua√ß√£o real no teste enviando para o Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"data/houses_test_raw.csv\")\n",
    "\n",
    "X_test_ids = X_test['Id'] # Keep ids\n",
    "X_test = X_test.drop(columns=['Id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando colunas ao X_test de acordo com o que fizemos no X\n",
    "X_test['sin_MoSold'] = np.sin(2 * np.pi * (X_test.MoSold - 1) / months_in_a_year)\n",
    "X_test['cos_MoSold'] = np.cos(2 * np.pi * (X_test.MoSold - 1) / months_in_a_year)\n",
    "\n",
    "X_test.drop(columns=['MoSold'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete_begin"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_stacking.fit(X, y_log)\n",
    "\n",
    "predictions_log = pipe_stacking.predict(X_test)\n",
    "predictions = np.exp(predictions_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([X_test_ids, pd.Series(predictions, name=\"SalePrice\")], axis=1)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporte os resultados\n",
    "results.to_csv(\"data/submission_final.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora vamos para a nossa ponderada!\n",
    "\n",
    "1. Descreva aqui as diferen√ßas de treinamento que voc√™ encontrou entre os treinamentos de Ensemble. Passe por todos os m√©todos. \n",
    "2. Descreva as modifica√ß√µes que voc√™ porp√¥s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - ao analisar os diferentes treinamento de ensenble, e possivel indentificar as seguintes peculiriaridade de cada um:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos Lineares (Ridge):** Treinados com pipeline integrado de pr√©-processamento, preproc definido anteriormente no codigo, incluindo normaliza√ß√£o, para melhores dados, e gridsearch  para o ajuste de hiperpar√¢metros.\n",
    "\n",
    "**KNN (KNeighborsRegressor):** Destaca-se pela imputa√ß√£o inteligente e sele√ß√£o de caracter√≠sticas, proporcionando uma abordagem refinada para melhorar a precis√£o da previs√£o.\n",
    "\n",
    "**SVM (SVR):** Adota abordagens linear e rbf, incorporando escala e preenchimento de valores ausentes. Tambem possui ajuste de par√¢metros por GridSearchCV, buscando uma melhor sele√ß√£o de hiper parametros.\n",
    "\n",
    "**√Årvores (DecisionTreeRegressor):** Aplica um modelo de √°rvores de decis√£o simples, com controle de overfitting pela defini√ß√£o de profundidade m√°xima e amostras m√≠nimas por folha.\n",
    "\n",
    "**Floresta Aleat√≥ria (RandomForestRegressor):** Emprega m√∫ltiplas √°rvores para melhor o resultado, refinando cada vez mais as decis√µes , proporcionando estabilidade e qualidade nos resutlados.\n",
    "\n",
    "**Boosted Trees (AdaBoostRegressor e GradientBoostingRegressor):** Constr√≥i modelos sequenciais para corre√ß√£o de erros anteriores, otimizando hiperpar√¢metros por random search, buscando aprimorar a qualidade dos parametros utilizados.\n",
    "\n",
    "**XGBoost (XGBRegressor):** No c√≥digo fornecido, o XGBoost  √© integrado a um pipeline do Scikit-Learn,  (pipe_xgb), que combina o pr√©-processamento e o modelo XGBoost. Como destaque, vale mencionar √© capacidade de interromper o treinamento antecipadamente e avaliar o desempenho, oferecendo maior controle e flexibilidade no ajuste do modelo.\n",
    "\n",
    "**Stacking:** Combina opini√µes de diversos modelos usando VotingRegressor e StackingRegressor, simplificando o modelo final com regress√£o linear, visando diversidade, interpretabilidade e melhoria na performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Sugest√£o de melhoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma dica de melhoria seria aumentar a vari√°vel max_depth, no modelo de random forest, o que permitiria que o modelo ficar mais complexo e identificar mais padr√µes, consequentemente aumentando a qualidade do modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
